#spider 程序测试

# -*- coding:UTF-8 -*-
import scrapy

class ComicSpider(scrapy.Spider):

    name = "comic"
    allowed_domains = ['comic.kukudm.com']
    start_urls = ['http://comic.kukudm.com/comiclist/3/']

    def parse(self, response):
        link_urls = response.xpath('//dd/a[1]/@href').extract()
        dir_names = response.xpath('//dd/a[1]/text()').extract()
        for each_link in link_urls:
            print('http://comic.kukudm.com' + each_link)
        for each_name in dir_names:
            print(each_name)
            
  
  
Items 编写  
  
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html

import scrapy

class ComicItem(scrapy.Item):
    dir_name = scrapy.Field()
    link_url = scrapy.Field()
    img_url = scrapy.Field()
    image_paths = scrapy.Field()
    

#setting 编写

BOT_NAME = 'cartoon'

SPIDER_MODULES = ['cartoon.spiders']
NEWSPIDER_MODULE = 'cartoon.spiders'


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = 'cartoon (+http://www.yourdomain.com)'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

ITEM_PIPELINES = {
    'cartoon.pipelines.ComicImgDownloadPipeline': 1,
}

IMAGES_STORE = 'J:/火影忍者'

COOKIES_ENABLED = False

DOWNLOAD_DELAY = 0.25    # 250 ms of delay



#comic spider 编写



#章节名
        dir_names = hxs.xpath('//dd/a[1]/text()').extract()
        #保存章节链接和章节名
        for index in range(len(urls)):
            item = ComicItem()
            item['link_url'] = self.server_link + urls[index]
            item['dir_name'] = dir_names[index]
            items.append(item)

        #根据每个章节的链接，发送Request请求，并传递item参数
        for item in items[-13:-1]:
            yield scrapy.Request(url = item['link_url'], meta = {'item':item}, callback = self.parse2)

    #解析获得章节第一页的页码数和图片链接   
    def parse2(self, response):
        #接收传递的item
        item = response.meta['item']
        #获取章节的第一页的链接
        item['link_url'] = response.url
        hxs = Selector(response)
        #获取章节的第一页的图片链接
        pre_img_url = hxs.xpath('//script/text()').extract()
        #注意这里返回的图片地址,应该为列表,否则会报错
        img_url = [self.server_img + re.findall(self.pattern_img, pre_img_url[0])[0]]
        #将获取的章节的第一页的图片链接保存到img_url中
        item['img_url'] = img_url
        #返回item，交给item pipeline下载图片
        yield item
        #获取章节的页数
        page_num = hxs.xpath('//td[@valign="top"]/text()').re(u'共(\d+)页')[0]
        #根据页数，整理出本章节其他页码的链接
        pre_link = item['link_url'][:-5]
        for each_link in range(2, int(page_num) + 1):
            new_link = pre_link + str(each_link) + '.htm'
            #根据本章节其他页码的链接发送Request请求，用于解析其他页码的图片链接，并传递item
            yield scrapy.Request(url = new_link, meta = {'item':item}, callback = self.parse3)

    #解析获得本章节其他页面的图片链接
    def parse3(self, response):
        #接收传递的item
        item = response.meta['item']
        #获取该页面的链接
        item['link_url'] = response.url
        hxs = Selector(response)
        pre_img_url = hxs.xpath('//script/text()').extract()
        #注意这里返回的图片地址,应该为列表,否则会报错
        img_url = [self.server_img + re.findall(self.pattern_img, pre_img_url[0])[0]]
        #将获取的图片链接保存到img_url中
        item['img_url'] = img_url
        #返回item，交给item pipeline下载图片
        yield item
        
  #Pipelines编写
  
  # -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
from cartoon import settings
from scrapy import Request
import requests
import os


class ComicImgDownloadPipeline(object):

    def process_item(self, item, spider):
        #如果获取了图片链接，进行如下操作
        if 'img_url' in item:
            images = []
            #文件夹名字
            dir_path = '%s/%s' % (settings.IMAGES_STORE, item['dir_name'])
            #文件夹不存在则创建文件夹
            if not os.path.exists(dir_path):
                os.makedirs(dir_path)
            #获取每一个图片链接
            for image_url in item['img_url']:
                #解析链接，根据链接为图片命名
                houzhui = image_url.split('/')[-1].split('.')[-1]
                qianzhui = item['link_url'].split('/')[-1].split('.')[0]
                #图片名
                image_file_name = '第' + qianzhui + '页.' + houzhui
                #图片保存路径
                file_path = '%s/%s' % (dir_path, image_file_name)
                images.append(file_path)
                if os.path.exists(file_path):
                    continue
                #保存图片
                with open(file_path, 'wb') as handle:
                    response = requests.get(url = image_url)
                    for block in response.iter_content(1024):
                        if not block:
                            break
                        handle.write(block)
            #返回图片保存路径
            item['image_paths'] = images
        return item
